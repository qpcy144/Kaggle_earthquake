{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.signal import hilbert,convolve,hann\n",
    "     \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('precision',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('F:\\\\Qplus\\\\Kaggle\\\\train.csv',chunksize=5000000,iterator=True,\n",
    "                     dtype={'acoustic_data':np.int16,'time_to_failure':np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_many_features(xc,seg_id=0):\n",
    "    X=pd.DataFrame(index=[seg_id,])\n",
    "    zc=np.fft.fft(xc)\n",
    "    \n",
    "    X.loc[seg_id,'mean']=xc.mean()\n",
    "    X.loc[seg_id,'std']=xc.std()\n",
    "    X.loc[seg_id,'max']=xc.max()\n",
    "    X.loc[seg_id,'min']=xc.min()\n",
    "    \n",
    "    #FFT transform values\n",
    "    realFFT = np.real(zc)\n",
    "    imagFFT = np.imag(zc)\n",
    "    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n",
    "    X.loc[seg_id, 'Rstd'] = realFFT.std()\n",
    "    X.loc[seg_id, 'Rmax'] = realFFT.max()\n",
    "    X.loc[seg_id, 'Rmin'] = realFFT.min()\n",
    "    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n",
    "    X.loc[seg_id, 'Istd'] = imagFFT.std()\n",
    "    X.loc[seg_id, 'Imax'] = imagFFT.max()\n",
    "    X.loc[seg_id, 'Imin'] = imagFFT.min()\n",
    "    \n",
    "    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n",
    "    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n",
    "    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n",
    "    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n",
    "    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n",
    "    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n",
    "    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n",
    "    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n",
    "    \n",
    "    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n",
    "    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n",
    "    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n",
    "    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n",
    "    \n",
    "    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n",
    "    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n",
    "    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n",
    "    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n",
    "    \n",
    "    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n",
    "    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n",
    "    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n",
    "    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n",
    "    \n",
    "    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n",
    "    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n",
    "    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n",
    "    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n",
    "    \n",
    "    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n",
    "    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n",
    "    X.loc[seg_id, 'avg_diff'] = np.mean(np.diff(xc))\n",
    "    #X.loc[seg_id, 'avg_diff_rate'] = np.mean(np.nonzero((np.diff(xc) / xc[:-1]))[0]) #seems do not help\n",
    "    \n",
    "    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n",
    "    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n",
    "    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n",
    "    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n",
    "    \n",
    "    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n",
    "    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n",
    "    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n",
    "    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n",
    "    \n",
    "    X.loc[seg_id, 'trend'] = add_trend_feature(xc)\n",
    "    X.loc[seg_id, 'abs_trend'] = add_trend_feature(xc, abs_values=True)\n",
    "    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n",
    "    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n",
    "    \n",
    "    X.loc[seg_id, 'mad'] = xc.mad()\n",
    "    X.loc[seg_id, 'kurt'] = xc.kurtosis()\n",
    "    X.loc[seg_id, 'skew'] = xc.skew()\n",
    "    X.loc[seg_id, 'med'] = xc.median()\n",
    "    \n",
    "    X.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(xc)).mean()\n",
    "    X.loc[seg_id, 'Hann_window_mean'] = (convolve(xc, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta1_mean'] = classic_sta_lta(xc, 500, 10000).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta2_mean'] = classic_sta_lta(xc, 5000, 100000).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta3_mean'] = classic_sta_lta(xc, 3333, 6666).mean()\n",
    "    X.loc[seg_id, 'classic_sta_lta4_mean'] = classic_sta_lta(xc, 10000, 25000).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n",
    "    \n",
    "    ewma = pd.Series.ewm\n",
    "    X.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(xc, span=300).mean()).mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n",
    "    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n",
    "    \n",
    "    no_of_std = 2\n",
    "    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n",
    "    X.loc[seg_id,'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X.loc[seg_id,'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    \n",
    "    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n",
    "    X.loc[seg_id,'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X.loc[seg_id,'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n",
    "    \n",
    "    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n",
    "    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n",
    "    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n",
    "    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n",
    "    \n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = xc.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = xc.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        \n",
    "        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate features for a list of acoustic data\n",
    "def gen_features_list(acd_list):\n",
    "    train_features=pd.DataFrame()\n",
    "    for acd in acd_list:\n",
    "        train_features=train_features.append(create_many_features(acd),ignore_index=True)\n",
    "    #print(train_features.shape)\n",
    "    return train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_segments(acd_segments,ttf_segments,split_length):\n",
    "    length=len(acd_segments)\n",
    "    #split each complete segements into many small segments\n",
    "    split_segments_index=np.array_split(np.arange(length),length//split_length)\n",
    "    print('the length of this segments is %d'%len(split_segments_index))\n",
    "    acd_split=[]\n",
    "    ttf_split=pd.Series()\n",
    "    for indices in split_segments_index:\n",
    "        acd_split.append(pd.Series(acd_segments[indices]))\n",
    "        ttf_split=ttf_split.append(pd.Series(ttf_segments[indices][-1]),ignore_index=True)\n",
    "    #print(len(acd_split))\n",
    "    return acd_split,ttf_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_trend_feature(arr,abs_values=False):\n",
    "    index=np.arange(len(arr))\n",
    "    if abs_values:\n",
    "        arr=np.abs(arr)\n",
    "    lr=LinearRegression()\n",
    "    lr.fit(index.reshape(-1,1),arr)\n",
    "    return lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classic_sta_lta(x,length_sta,length_lta):\n",
    "    sta=np.cumsum(x**2)\n",
    "    sta=np.require(sta,dtype=np.float)\n",
    "    \n",
    "    lta=sta.copy()\n",
    "    \n",
    "    sta[length_sta:]=sta[length_sta:]-sta[:-length_sta]\n",
    "    sta/=length_sta\n",
    "    \n",
    "    lta[length_lta:]=lta[length_lta:]-lta[:-length_lta]\n",
    "    lta/=length_lta\n",
    "    \n",
    "    sta[:length_sta-1]=0\n",
    "    \n",
    "    dtiny=np.finfo(0.0).tiny\n",
    "    idx=lta<dtiny\n",
    "    lta[idx]=dtiny\n",
    "    return sta/lta\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[656573.]\n",
      "the length of this segments is 56\n",
      "1\n",
      "[85877.]\n",
      "the length of this segments is 444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_X=pd.DataFrame()\n",
    "train_y=pd.Series()\n",
    "acd_to_be_extended=np.array([])\n",
    "ttf_to_be_extended=np.array([])\n",
    "last=math.inf\n",
    "split_length=100000\n",
    "for chunk in train_df:\n",
    "    acd=chunk.acoustic_data.values\n",
    "    ttf=chunk.time_to_failure.values\n",
    "    #split_index=[]   #record the index for segmentation\n",
    "    split_index=np.array([])\n",
    "    if ttf[0]>last:\n",
    "        #split_index.append[0]\n",
    "        acd_split,ttf_split=split_segments(acd_to_be_extended,ttf_to_be_extended,split_length)      \n",
    "        train_X=train_X.append(gen_features_list(acd_split),ignore_index=True)\n",
    "        train_y=train_y.append(ttf_split,ignore_index=True)\n",
    "        acd_to_be_extended=np.array([])\n",
    "        ttf_to_be_extended=np.array([])\n",
    "        \n",
    "    find_split=ttf[1:]>ttf[:-1]\n",
    "    #split_index.append(np.where(find_split))\n",
    "    split_index=np.append(split_index,np.where(find_split))\n",
    "    length=len(split_index.tolist())\n",
    "    if length!=0:     #which means a segment split exists\n",
    "        print(length)\n",
    "        print(split_index)\n",
    "        acd_to_be_extended=np.append(acd_to_be_extended,\n",
    "                                     acd[:int(split_index[0]+1)])\n",
    "        ttf_to_be_extended=np.append(ttf_to_be_extended,ttf[:int(split_index[0]+1)])\n",
    "        acd_split,ttf_split=split_segments(acd_to_be_extended,ttf_to_be_extended,split_length)\n",
    "        train_X=train_X.append(gen_features_list(acd_split),ignore_index=True)\n",
    "        train_y=train_y.append(ttf_split,ignore_index=True)\n",
    "        #print(train_X.describe())\n",
    "        #print(train_y.describe())\n",
    "        acd_to_be_extended=np.array([])\n",
    "        ttf_to_be_extended=np.array([])\n",
    "        \n",
    "        for i in range(length-1):\n",
    "            acd_to_be_extended=acd[int(split_index[i])+1:int(split_index[i+1])+1]\n",
    "            ttf_to_be_extended=ttf[int(split_index[i])+1:int(split_index[i+1])+1]\n",
    "            acd_split,ttf_split=split_segments(acd_to_be_extended,ttf_to_be_extended,split_length)\n",
    "            train_X=train_X.append(gen_features_list(acd_split),ignore_index=True)\n",
    "            train_y=train_y.append(ttf_split,ignore_index=True)\n",
    "            acd_to_be_extended=np.array([])\n",
    "            ttf_to_be_extended=np.array([])\n",
    "        acd_to_be_extended=acd[int(split_index[-1]):]\n",
    "        ttf_to_be_extended=ttf[int(split_index[-1]):]\n",
    "        \n",
    "    else:\n",
    "        acd_to_be_extended=np.append(acd_to_be_extended,acd)\n",
    "        ttf_to_be_extended=np.append(ttf_to_be_extended,ttf)\n",
    "    last=ttf[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X_seg_ids=pd.read_csv('F:\\\\Qplus\\\\Kaggle\\\\sample_submission.csv',dtype={'time_to_failure':np.float32},index_col='seg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X_seg_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X=pd.DataFrame()\n",
    "for seg_id in test_X_seg_ids.index.values:\n",
    "    #print(type(seg_id))\n",
    "    seg_data=pd.read_csv('F:\\\\Qplus\\\\Kaggle\\\\test\\\\'+seg_id+'.csv',dtype={'acoustic_data':np.float32})\n",
    "    test_X=test_X.append(create_many_features(seg_data.acoustic_data,seg_id=seg_id),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Merged_X=pd.concat((train_X,test_X),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(Merged_X)\n",
    "train_X_scaled=scaler.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X_scaled=scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_transformed=train_X_scaled.reshape(train_X_scaled.shape[0],train_X_scaled.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM_model=Sequential()\n",
    "LSTM_model.add(LSTM(100,input_shape=(train_X_transformed.shape[1],train_X_transformed.shape[2])))\n",
    "LSTM_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_model.compile(optimizer='adam',loss='mae')\n",
    "history=LSTM_model.fit(train_X_transformed,train_y,epochs=200,batch_size=64,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X_scaled_transformed = test_X_scaled.reshape(test_X_scaled.shape[0],\n",
    "                                                  test_X_scaled.shape[1], 1)\n",
    "y_predict = LSTM_model.predict(test_X_scaled_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('F:\\\\Qplus\\\\Kaggle\\\\sample_submission.csv', index_col='seg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('F:\\\\Qplus\\\\Kaggle\\\\submission.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 we will use LSTM model first to predict time_to_failure\n",
    "# monitored criterion should in the metrics list for model's compile function\n",
    "callbacks_earlystopping=[keras.callbacks.EarlyStopping(patience=20,monitor='val_loss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_model=Sequential()\n",
    "LSTM_model.add(LSTM(100,input_shape=(train_X_transformed.shape[1],train_X_transformed.shape[2])))\n",
    "LSTM_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LSTM_model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])\n",
    "history=LSTM_model.fit(train_X_transformed,train_y,epochs=500,batch_size=64,\n",
    "                       validation_split=0.25,callbacks=callbacks_earlystopping,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.legend(['val_loss','loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.legend(['val_mae','mae'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca=PCA()\n",
    "pca.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#obviously only the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca=PCA()\n",
    "pca.fit(train_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from the pca analysis, we will only use the first 32 components\n",
    "pca=PCA(n_components=32)\n",
    "X_dim_reduced=pca.fit_transform(train_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_dim_reduced_transformed=X_dim_reduced.reshape(X_dim_reduced.shape[0],X_dim_reduced.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 we will use dimensions reduced by PCA, then apply LSTM model to predict time_to_failure\n",
    "# monitored criterion should in the metrics list for model's compile function\n",
    "callbacks_earlystopping=[keras.callbacks.EarlyStopping(patience=20,monitor='val_loss')]\n",
    "LSTM_model=Sequential()\n",
    "LSTM_model.add(LSTM(100,input_shape=(X_dim_reduced_transformed.shape[1],\n",
    "                                     X_dim_reduced_transformed.shape[2])))\n",
    "LSTM_model.add(Dense(1))\n",
    "LSTM_model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])\n",
    "history=LSTM_model.fit(X_dim_reduced_transformed,train_y,epochs=500,batch_size=64,\n",
    "                       validation_split=0.25,callbacks=callbacks_earlystopping,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.legend(['val_loss','loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X_seg_ids=pd.read_csv('F:\\\\Qplus\\\\Kaggle\\\\sample_submission.csv',\n",
    "                           dtype={'time_to_failure':np.float32},index_col='seg_id')\n",
    "test_X=pd.DataFrame()\n",
    "for seg_id in test_X_seg_ids.index.values:\n",
    "    #print(type(seg_id))\n",
    "    seg_data=pd.read_csv('F:\\\\Qplus\\\\Kaggle\\\\test\\\\'+seg_id+'.csv',\n",
    "                         dtype={'acoustic_data':np.float32})\n",
    "    test_X=test_X.append(create_many_features(seg_data.acoustic_data,seg_id=seg_id),\n",
    "                         ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "scaler.fit(Merged_X)\n",
    "train_X_scaled=scaler.transform(train_X)\n",
    "test_X_scaled=scaler.transform(test_X)\n",
    "test_X_scaled_transformed = test_X_scaled.reshape(test_X_scaled.shape[0],\n",
    "                                                  test_X_scaled.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test data should go through the same PCA reduction process\n",
    "pca=PCA(n_components=32)\n",
    "test_X_dim_reduced=pca.fit_transform(test_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X_dim_reduced_transformed= test_X_dim_reduced.reshape(test_X_dim_reduced.shape[0],\n",
    "                                                  test_X_dim_reduced.shape[1], 1)\n",
    "predict_y=LSTM_model.predict(test_X_dim_reduced_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = predict_y\n",
    "submission.to_csv('F:\\\\Qplus\\\\Kaggle\\\\submission_1.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use SVR for prediction\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr=SVR(kernel='rbf')\n",
    "svr.fit(train_X_scaled,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svr_parameters={'kernel':['linear','rbf'],\n",
    "                'gamma':[1e-4, 0.001, 0.01, 'auto'],\n",
    "                'C':[0.1, 0.2,0.5,1,3,5],\n",
    "               'epsilon':[0.5,1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr=SVR()\n",
    "gcv=GridSearchCV(svr,param_grid=svr_parameters,scoring='neg_mean_absolute_error')\n",
    "gcv.fit(train_X_scaled,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr=SVR()\n",
    "gcv=GridSearchCV(svr,param_grid=svr_parameters)\n",
    "gcv.fit(train_X_scaled,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the best parameter for gamma is 'auto' (this default parameter means that gamma=1/n_features), so the gridsearch should include the search of 'auto'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gcv.predict(test_X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = gcv.predict(test_X_scaled)\n",
    "submission.to_csv('F:\\\\Qplus\\\\Kaggle\\\\submission_2.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
